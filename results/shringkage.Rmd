---
title: "shringkage"
author: "Taraneh"
date: "2024-02-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
#### model selection and shrinkage


	####   Subset selection methods
	##### 


	## data set Hitters.  response variable is Salary.

	library (ISLR)
	fix(Hitters )   ## show data in excel form
	names(Hitters )


	sum(is.na(Hitters$Salary))   ##  is.na()   is used to identify missing values.  



	Hitters =na.omit(Hitters )    ##  na.omit():  remove observations with missing values.
	dim(Hitters )
	
	sum(is.na(Hitters ))


	##  regsubsets():  perform best subset selection by identifying the best model that contains a given number of predictors.


	install.packages("leaps")
	library (leaps)
	regfit.full=regsubsets (Salary~.,Hitters )
	summary(regfit.full)     ## asterisk indicates that a given variable is included in the correspondingmodel


	regfit.full=regsubsets (Salary~.,data=Hitters ,nvmax =19)   ## nvmax=:  specify the maximum number of predictors in the model
	reg.summary =summary(regfit.full) 


	names(reg.summary )   ## show the names of the output in reg.summary


	reg.summary$rsq
	reg.summary$cp



	par(mfrow =c(2,2))
	plot(reg.summary$rss ,xlab=" Number of Variables ",ylab=" RSS",type="l")  ## plot RSS against the number of variables.
	plot(reg.summary$adjr2 ,xlab =" Number of Variables ",ylab=" Adjusted RSq",type="l")   ##  plot Adjusted R^2



	which.max(reg.summary$adjr2)   ## identify the model with the largest adjusted R^2

	points(11, reg.summary$adjr2[11], col ="red",cex =2, pch =20)   ## show this model on the plot previously obtained.




	plot(reg.summary$cp,xlab ="Number of Variables ",ylab="Cp",type="l")   ## plot C_p
	which.min(reg.summary$cp )

	points(10, reg.summary$cp[10], col ="red",cex =2, pch =20)
	which.min(reg.summary$bic )

	plot(reg.summary$bic ,xlab=" Number of Variables ",ylab="BIC",type="l")   ## plot BIC
	points(6, reg.summary$bic[6], col ="red",cex =2, pch =20)


	## display the selected variables for the best model with a given number of predictors
	plot(regfit.full ,scale ="r2")
	plot(regfit.full ,scale ="adjr2")
	plot(regfit.full ,scale ="Cp")
	plot(regfit.full ,scale ="bic")


```

```{r}
###########################################
####   Ridge Regression and the Lasso
##########################################

	
	## ridge regression
	#use glmnet package and Hitters data set

	install.packages("glmnet")
	library(glmnet)


	x=model.matrix(Salary~.,Hitters)[,-1]   ### model.matrix():  get the input variables as a matrix.
	y=Hitters$Salary					## missing values should be removed

	grid =10^seq(10,-2, length =100)

	## glmnet(): alpha=0 is ridge regression;  alpha=1 is lasso regression
	## by default, the X is standardized.
	ridge.mod =glmnet(x,y,alpha =0, lambda =grid)  ## the input variables must be a matrix X.
	dim(coef(ridge.mod ))   ## coefficients for 100 lambda

	ridge.mod$lambda[50]  ## l2 norm of coefficient corresponds to lambda[50]
	coef(ridge.mod)[,50]
	sqrt(sum(coef(ridge.mod)[ -1 ,50]^2) )

	ridge.mod$lambda [60]
	coef(ridge.mod)[,60]
	sqrt(sum(coef(ridge.mod)[ -1 ,60]^2) )   ## norm is larger than lambda[50]

	predict (ridge.mod ,s=50, type ="coefficients")[1:20 ,]  ## use the model with lambda[50] to predict



	#### use test data to evaluate test error of ridge regression and lasso

	set.seed (1)
	train=sample (1: nrow(x), nrow(x)/2)
	test=(- train )
	y.test=y[test]

	ridge.mod =glmnet (x[train ,],y[train],alpha =0, lambda =grid ,thresh =1e-12)
	ridge.pred=predict (ridge.mod ,s=4, newx=x[test ,])
	mean(( ridge.pred -y.test)^2)

	ridge.pred=predict(ridge.mod ,s=1e10 ,newx=x[test ,])   ## use different lambda
	mean(( ridge.pred -y.test)^2)


	ridge.pred=predict(ridge.mod,s=0, newx=x[test ,])  ### s=0:  least square estimate
	 mean(( ridge.pred -y.test)^2)



	## CV to select best lambda
	set.seed (1)
	cv.out =cv.glmnet (x[train ,],y[train],alpha =0)
	plot(cv.out)
	bestlam =cv.out$lambda.min
	bestlam

	# use the best lambda to predict
	ridge.pred=predict (ridge.mod ,s=bestlam ,newx=x[test ,])
	mean(( ridge.pred -y.test)^2)

	## use the best lambda to full data set
	out=glmnet (x,y,alpha =0)
	predict (out ,type="coefficients",s=bestlam )[1:20 ,]




	## lasso 
	lasso.mod =glmnet(x[train ,],y[train],alpha =1, lambda =grid) ### note: alpha=1 corresponds to lasso

	plot(lasso.mod)




	set.seed (1)
	cv.out =cv.glmnet(x[train ,],y[train],alpha =1)  ### note: alpha=1 corresponds to lasso
	plot(cv.out)
	bestlam =cv.out$lambda.min
	lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test ,])
	mean(( lasso.pred -y.test)^2)

	

```







options(repos = c(CRAN = "http://cran.rstudio.com"))
install.packages("leaps")
library(leaps)
install.packages("leaps")


